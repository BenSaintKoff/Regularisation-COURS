{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP PRATIQUE SUR LA REGULARISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous examinerons la mise en œuvre de différentes techniques de régularisation. Tout d’abord, nous commencerons par la régression linéaire multiple. Pour cela, nous avons besoin de l’environnement python3 avec sci-kit learn et pandas\n",
    "Premièrement, nous devons importer des bibliothèques dans notre environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser l’ensemble de données de prédiction de Boston. Cet ensemble de données est présent dans le module des ensembles de données de la bibliothèque sklearn (scikit-learn). Nous pouvons importer ce jeu de données comme suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre-defined Boston Dataset\n",
    "boston_dataset = datasets.load_boston()\n",
    "print(boston_dataset.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons conclure de la description ci-dessus que nous avons 13 variables indépendantes et une variable dépendante (prix de la maison). Nous devons maintenant vérifier une corrélation entre variables indépendantes et variable dépendante. Nous pouvons utiliser scatterplot/corrplot pour cela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatter plot of independent vs Dependent variable\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize = (18, 18))\n",
    "\n",
    "for index, feature_name in enumerate(boston_dataset.feature_names):\n",
    "    ax = fig.add_subplot(4, 4, index + 1)\n",
    "    ax.scatter(boston_dataset.data[:, index], boston_dataset.target)\n",
    "    ax.set_ylabel('House Price', size = 12)\n",
    "    ax.set_xlabel(feature_name, size = 12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessus produit des diagrammes de dispersion de différentes variables indépendantes avec la variable cible comme indiqué ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer à partir des graphiques de dispersion ci-dessus que certaines des variables indépendantes ne sont pas très corrélées (positivement ou négativement) avec la variable cible. Ces variables verront leurs coefficients être réduits en régularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into Pandas Dataframe\n",
    "boston_pd = pd.DataFrame(boston_dataset.data)\n",
    "boston_pd.columns = boston_dataset.feature_names\n",
    "boston_pd_target = np.asarray(boston_dataset.target)\n",
    "boston_pd['House Price'] = pd.Series(boston_pd_target)\n",
    "\n",
    "# input \n",
    "X = boston_pd.iloc[:, :-1]\n",
    "\n",
    "#output\n",
    "Y = boston_pd.iloc[:, -1]\n",
    "\n",
    "print(boston_pd.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous appliquons la division train-test pour diviser l’ensemble de données en deux parties, l’une pour l'apprentissage et l’autre pour le test. Nous utiliserons 25 % des données pour les essais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(boston_pd.iloc[:, :-1], boston_pd.iloc[:, -1], test_size = 0.25)\n",
    "\n",
    "print(\"Train data shape of X = % s and Y = % s : \"%(x_train.shape, y_train.shape))\n",
    "print(\"Test data shape of X = % s and Y = % s : \"%(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression multiple (linéaire) \n",
    "C’est maintenant le bon moment pour tester les modèles. Nous utiliserons d’abord la régression linéaire multiple. Nous formons le modèle sur les données de formation et calculons le MSE sur test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple Linear Regression Model\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(x_train, y_train)\n",
    "\n",
    "# Generate Prediction on test set\n",
    "lreg_y_pred = lreg.predict(x_test)\n",
    "\n",
    "# calculating Mean Squared Error (mse)\n",
    "mean_squared_error = np.mean((lreg_y_pred - y_test)**2)\n",
    "print(\"Mean squared Error on test set : \", mean_squared_error)\n",
    "\n",
    "# Putting together the coefficient and their corresponding variable names \n",
    "lreg_coefficient = pd.DataFrame()\n",
    "lreg_coefficient[\"Columns\"] = x_train.columns\n",
    "lreg_coefficient['Coefficient Estimate'] = pd.Series(lreg.coef_)\n",
    "print(lreg_coefficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traçons un graphique à barres des coefficients ci-dessus en utilisant matplotlib bibliothèque de visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the coefficient score\n",
    "fig, ax = plt.subplots(figsize =(20, 10))\n",
    "\n",
    "color =['tab:gray', 'tab:blue', 'tab:orange', \n",
    "'tab:green', 'tab:red', 'tab:purple', 'tab:brown', \n",
    "'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan', \n",
    "'tab:orange', 'tab:green', 'tab:blue', 'tab:olive']\n",
    "\n",
    "ax.bar(lreg_coefficient[\"Columns\"], \n",
    "lreg_coefficient['Coefficient Estimate'], \n",
    "color = color)\n",
    "\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons observer que beaucoup de variables ont un coefficient insignifiant, ces coefficients n’ont pas beaucoup contribué au modèle et doivent réguler ou même éliminer certaines de ces variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression de Ridge : \n",
    "Ridge Regression a ajouté un terme dans la fonction d’erreur de moindre carré ordinaire qui régularise la valeur des coefficients des variables. Ce terme est la somme des carrés de coefficient multipliés par le paramètre Le motif de l’ajout de ce terme est de pénaliser la variable correspondant à ce coefficient peu corrélé à la variable cible. Ce terme est appelé régularisation L2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ridge regression from sklearn library\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train the model \n",
    "ridgeR = Ridge(alpha = 1)\n",
    "ridgeR.fit(x_train, y_train)\n",
    "y_pred = ridgeR.predict(x_test)\n",
    "\n",
    "# calculate mean square error\n",
    "mean_squared_error_ridge = np.mean((y_pred - y_test)**2)\n",
    "print(mean_squared_error_ridge)\n",
    "\n",
    "# get ridge coefficient and print them\n",
    "ridge_coefficient = pd.DataFrame()\n",
    "ridge_coefficient[\"Columns\"]= x_train.columns\n",
    "ridge_coefficient['Coefficient Estimate'] = pd.Series(ridgeR.coef_)\n",
    "print(\"The value of MSE error and the dataframe with ridge coefficients. \")\n",
    "print(ridge_coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The bar plot of above data is: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessus, nous prenons $\\alpha = 1$. \n",
    "Regardons un autre graphique à barres avec $\\alpha = 10$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons l’observer à partir des graphiques ci-dessus, l’$\\alpha$ aide à régulariser le coefficient et à les faire converger plus rapidement. \n",
    "\n",
    "Notez que les graphiques ci-dessus peuvent être trompeurs d’une manière qui montre que certains des coefficients deviennent nuls. En régularisation de **Ridge**, les coefficients ne peuvent jamais être 0, ils sont tout simplement trop petits pour être observés dans les parcelles ci-dessus. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Régression au lasso : \n",
    "La régression de Lasso est similaire à la régression de Ridge sauf que nous ajoutons ici la valeur absolue moyenne des coefficients à la place de la valeur carrée moyenne. Contrairement à la régression de crête, la régression de Lasso peut complètement éliminer la variable en réduisant sa valeur de coefficient à 0. Le nouveau terme que nous avons ajouté à Ordinary Least Square (OLS) est appelé régularisation L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Lasso regression from sklearn library\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train the model\n",
    "lasso = Lasso(alpha = 1)\n",
    "lasso.fit(x_train, y_train)\n",
    "y_pred1 = lasso.predict(x_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mean_squared_error = np.mean((y_pred1 - y_test)**2)\n",
    "print(\"Mean squared error on test set\", mean_squared_error)\n",
    "lasso_coeff = pd.DataFrame()\n",
    "lasso_coeff[\"Columns\"] = x_train.columns\n",
    "lasso_coeff['Coefficient Estimate'] = pd.Series(lasso.coef_)\n",
    "\n",
    "print(\"The value of MSE error and the dataframe with Lasso coefficients. :\")\n",
    "print(lasso_coeff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The bar plot of above coefficients: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression de Lasso a donné le même résultat que la régression de Ridge, lorsque nous augmentons la valeur d’alpha . Regardons un autre graphique à $\\alpha = 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net : \n",
    "Dans la régularisation Elastic Net, nous avons ajouté les termes L1 et L2 pour obtenir la fonction de perte finale. Cela nous amène à réduire la fonction de perte suivante : \n",
    "$$ L_{elastic-Net} \\left (  \\hat\\beta\\right )=  \\left (\\sum \\left ( y - x_{i}^{J} \\hat{ \\beta}  \\right ) 2 \\right )/2n+ \\lambda \\left ( (1 - \\alpha )/2 *  \\sum_{j=1} {m}  \\hat{ \\beta_{j} {2}} + \\alpha *  \\sum_{j=1} {m}  \\left  |  \\hat{ \\beta_{j}}  \\right  |  \\right)$$\n",
    "où alpha est entre 0 et 1. quand alpha = 1, Il réduit la durée de pénalité à L1 et si $alpha = 0$, il réduit ce terme à L2 \n",
    "pénalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Train the model\n",
    "e_net = ElasticNet(alpha = 1)\n",
    "e_net.fit(x_train, y_train)\n",
    "\n",
    "# calculate the prediction and mean square error\n",
    "y_pred_elastic = e_net.predict(x_test)\n",
    "mean_squared_error = np.mean((y_pred_elastic - y_test)**2)\n",
    "print(\"Mean Squared Error on test set\", mean_squared_error)\n",
    "\n",
    "e_net_coeff = pd.DataFrame()\n",
    "e_net_coeff[\"Columns\"] = x_train.columns\n",
    "e_net_coeff['Coefficient Estimate'] = pd.Series(e_net.coef_)\n",
    "e_net_coeff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : \n",
    "L’analyse ci-dessus permet de tirer la conclusion suivante sur les différentes méthodes de régularisation : \n",
    " \n",
    "\n",
    "    La régularisation est utilisée pour réduire la dépendance à une variable indépendante particulière en ajoutant le terme de pénalité à la fonction Perte. Ce terme empêche les coefficients des variables indépendantes de prendre des valeurs extrêmes. \n",
    "     \n",
    "    Ridge Regression ajoute le terme de pénalité de régularisation L2 à la fonction de perte. Ce terme réduit les coefficients mais ne les rend pas 0 et n’élimine donc pas complètement toute variable indépendante. Il peut être utilisé pour mesurer l’impact des différentes variables indépendantes. \n",
    "     \n",
    "    La régression Lasso ajoute le terme de pénalité de régularisation L1 à la fonction de perte. Ce terme réduit les coefficients et les rend 0, éliminant ainsi complètement la variable indépendante correspondante. Il peut être utilisé pour la sélection des fonctionnalités, etc. \n",
    "     \n",
    "    Elastic Net est une combinaison des deux régularisations ci-dessus. Il contient à la fois la L1 et la L2 comme terme de pénalité. Il fonctionne mieux que Ridge et Lasso régression pour la plupart des cas de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
